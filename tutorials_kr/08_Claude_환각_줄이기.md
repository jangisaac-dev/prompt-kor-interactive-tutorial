## 1. Claude의 창의적인 거짓말, 어떻게 멈출까?

Claude와 같은 대규모 언어 모델(LLM)을 사용하다 보면, 분명히 모를 법한 질문에도 마치 사실인 양 아주 구체적이고 당당하게 답변하는 모습을 보게 됩니다. 

"역사상 가장 무거운 하마의 이름은 무엇인가요?"라는 질문에 Claude가 "1950년대 런던 동물원에 있었던 '험프리(Humphrey)'입니다"라고 답한다면, 여러분은 믿으시겠습니까? 하지만 '험프리'라는 하마는 Claude가 당신을 돕기 위해 즉석에서 지어낸 가공의 존재일 가능성이 높습니다. 

이것이 바로 AI의 **환각(Hallucination)**, 또는 **작화(Confabulation)** 현상입니다. 오늘은 Claude의 '과한 친절'을 제어하고, 신뢰성 있는 RAG 시스템의 초석이 되는 환각 방지 전략을 딥다이브해 봅니다.

## 2. 왜 AI는 거짓말을 할까? (Helpfulness vs Truthfulness)

환각이 발생하는 근본적인 이유는 모델의 훈련 목표에 있습니다. Claude는 기본적으로 사용자에게 **'도움이 되는(Helpful)'** 답변을 하도록 설계되었습니다.

문제는 모델이 답변을 생성할 때, '진실성(Truthfulness)'보다 '도움이 되는 태도'를 우선시하는 경향이 있다는 점입니다. 학습 데이터에 없는 정보라도 질문의 맥락에 어울리는 '가장 확률 높은 다음 단어'를 이어 붙이다 보니, 존재하지 않는 하마 '험프리'가 탄생하게 되는 것입니다.

## 3. 전략 1: 명시적인 '퇴로' 제공 (The "Out" Option)

환각을 줄이는 가장 간단하면서도 강력한 기법은 Claude에게 **"모르면 모른다고 말해도 된다"**는 허가를 주는 것입니다. 이를 **"Out" 전략**이라고 부릅니다.

| 유형 | 프롬프트 예시 |
| --- | --- |
| **나쁜 예시 (압박형)** | "역사상 가장 무거운 하마의 이름은 무엇인가요?" |
| **좋은 예시 (퇴로 제공)** | "역사상 가장 무거운 하마의 이름은 무엇인가요? **만약 확실한 답을 모른다면, 추측하거나 지어내지 말고 '모르겠습니다'라고 답변하세요.**" |

**결과 비교:**
- **압박형:** (험프리와 같은 가공의 이름을 생성)
- **퇴로 제공:** "기록상 가장 무거운 하마의 특정 이름에 대해서는 제가 학습한 데이터에 정보가 없습니다. 일반적으로 하마는 1,500~3,000kg까지 나갑니다."

## 4. 전략 2: 근거 기반 답변 (Quotes First)

긴 문서 분석이나 RAG(Retrieval-Augmented Generation) 시스템에서 가장 치명적인 것은 문서에 없는 내용을 모델이 가진 일반 상식으로 채우는 것입니다. 이를 방지하기 위해 **"먼저 인용하고, 그 다음에 답변하라"**는 지시를 내립니다.

### Matterport SEC Filing 케이스 스터디
실제 사례를 보겠습니다. Matterport의 2021년 실적 보고서(10-K) 데이터를 컨텍스트로 제공하고, 문서에 없는 정보를 물어보는 상황입니다.

**제공된 데이터:**
> "2021년 5월 31일 기준, 당사는 약 503,000명의 구독자를 보유하고 있으며, 이는 2020년 5월 31일의 254,000명 대비 98% 증가한 수치입니다."

**함정 질문:** "2019년 5월 31일 기준 구독자 수는 얼마인가요?"

이때 단순히 답을 구하면 Claude는 추세를 계산해서 2019년 수치를 '추측'해버릴 수 있습니다. 이를 방지하는 프롬프트 구조는 다음과 같습니다.

```markdown
아래 문서를 읽고 질문에 답하세요.

<document>
[Matterport SEC Filing 데이터]
</document>

질문: Matterport의 2019년 5월 31일 기준 구독자 수는 얼마인가요?

지침:
1. 답변하기 전에, 질문과 관련된 구절을 문서에서 찾아 <relevant_quotes> 태그 안에 그대로 인용하세요.
2. 만약 문서에서 관련 구절을 찾을 수 없다면 <relevant_quotes>인용할 내용 없음</relevant_quotes>이라고 적으세요.
3. 이후 인용된 내용만을 바탕으로 답변하세요. 문서에 답이 없다면 "문서에서 정보를 찾을 수 없습니다"라고 답하세요.
```

> ⚠️ 아래 내용은 실제 실행 결과가 아닌 텍스트 시뮬레이션입니다.

```text
<relevant_quotes>
인용할 내용 없음
</relevant_quotes>

제공된 문서에는 2019년 5월 31일 기준 구독자 수에 대한 정보가 포함되어 있지 않습니다. 문서에는 2021년과 2020년의 데이터만 존재합니다.
```

## 5. Deep Dive: 왜 '인용 우선'이 작동할까?

이 기법이 환각을 획기적으로 줄여주는 이유는 모델의 **Attention(주의)** 메커니즘과 밀접한 관련이 있습니다.

1.  **주의의 고정 (Attention Focusing):** 답변 전에 인용문을 먼저 출력하게 하면, 모델의 연산 자원(Attention)이 전체 문서 중 질문과 관련된 특정 영역에 강하게 고정됩니다.
2.  **지식 충돌 해소:** 모델이 가진 '내부 지식(학습된 데이터)'과 '외부 컨텍스트(주어진 문서)'가 부딪힐 때, 인용 과정을 통해 외부 컨텍스트에 절대적인 가중치를 부여하도록 강제합니다.
3.  **답변의 근거화 (Grounding):** 인용문을 생성하는 행위 자체가 모델에게는 "이 텍스트 블록 안에서만 움직여라"라는 강력한 가이드라인이 됩니다.

## 6. 결론: 신뢰할 수 있는 AI를 위한 첫걸음

환각을 줄이는 것은 단순히 프롬프트 기술을 넘어, 사용자가 AI를 얼마나 신뢰할 수 있느냐의 문제입니다. 특히 금융, 법률, 의료와 같이 팩트가 중요한 도메인에서는 위에서 언급한 **퇴로 제공(The Out)**과 **인용 우선(Quotes First)** 전략이 필수적입니다.

이러한 기법들은 현대 AI 아키텍처의 핵심인 **RAG(검색 증강 생성)**의 가장 기초적인 품질 제어 장치가 됩니다.

## TL;DR
*   **환각(Hallucination):** 모델이 진실보다 '도움이 되는 태도'를 우선시하여 정보를 지어내는 현상.
*   **Out 전략:** "모르면 모른다고 하라"는 지시만으로도 가공의 답변을 대폭 차단할 수 있음.
*   **인용 우선(Quotes First):** `<relevant_quotes>` 태그를 사용하여 답변 전 근거 문구를 먼저 추출하게 함.
*   **핵심 원리:** 모델의 Attention을 컨텍스트에 고정시켜 내부 지식의 개입을 막는 Grounding 기법.

## 참고 링크
*   [Anthropic Cookbook: Avoiding Hallucinations](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/classification/guide.ipynb)
*   [Claude Documentation: Reduce Hallucinations](https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations)

Claude, 프롬프트엔지니어링, 환각줄이기, RAG, 인공지능신뢰성, LLM가이드, Anthropic, AI윤리